{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bài tập 1\n",
    "\n",
    "\n",
    "Phan Bá Đức - 22120071\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.\n",
    "**Explaination**:\n",
    "\n",
    "(i) The developers already know the exact coin specifications. Therefore, when coins are inserted into the vending machine, they only need to read/calculate the parameters of the coin and then use a conditional statement to classify the coins. => There is no learning involved.\n",
    "\n",
    "(ii) Initially, the developers didn't know the coin specifications. They provided the program/algorithm with a large set of labeled data (labeled coins) so that the program could learn the differences between each label and use that knowledge to classify the coins. => Supervised Learning.\n",
    "\n",
    "(iii) The computer develops a strategy through practice, adjusting its actions based on penalties and rewards. => Reinforcement Learning.\n",
    "\n",
    "*Therefore, the correct answer is*: [d] (i) Not learning, (ii) Supervised Learning, (iii) Reinforcement Learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\n",
    "\n",
    "**Explaination**:\n",
    "\n",
    "(i) There are many mathematical formulas or algorithms to determine prime numbers. → No need for machine learning.\n",
    "\n",
    "(ii) There are no existing templates or formulas to detect fraud, so the computer must learn from historical data on previous fraud cases to identify patterns. → This requires machine learning.\n",
    "\n",
    "(iii) The formula for calculating the time it takes for a falling object to hit the ground is well-known in physics, so no machine learning is required. → No need for machine learning.\n",
    "\n",
    "(iv) Similar to (ii), traffic patterns are unpredictable. The computer must learn from available data and adjust its decisions based on specific situations without human intervention. → This requires machine learning.\n",
    "\n",
    "*Therefore, the correct answer is* [a] (ii) and (iv).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.\n",
    "\n",
    "**Explaination**:\n",
    "\n",
    "We have two bags:\n",
    "\n",
    "- **Bag 1**: Contains two black balls.\n",
    "- **Bag 2**: Contains one black ball and one white ball.\n",
    "\n",
    "We define the following events:\n",
    "\n",
    "- **A**: The event of selecting the first black ball.\n",
    "- **B**: The event of selecting the second black ball.\n",
    "\n",
    "#### **Probability of selecting the first black ball**\n",
    "\n",
    "- **For Bag 1**:  \n",
    "  The probability of choosing **Bag 1** is $\\frac{1}{2}$. Since **Bag 1** contains two black balls, the probability of selecting the first black ball from **Bag 1** is:\n",
    "  \n",
    "$$\n",
    "P(A_1) = \\frac{1}{2} \\times 1 = \\frac{1}{2}\n",
    "$$\n",
    "  \n",
    "\n",
    "- **For Bag 2**:  \n",
    "  The probability of choosing **Bag 2** is \\(\\frac{1}{2}\\), and since **Bag 2** contains one black ball and one white ball, the probability of selecting the first black ball from **Bag 2** is:\n",
    "  \n",
    "$$\n",
    "P(A_2) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4}\n",
    "$$\n",
    "  \n",
    "\n",
    "#### **Total probability of selecting the first black ball**\n",
    "\n",
    "The total probability of selecting the first black ball is the sum of the probabilities for **Bag 1** and **Bag 2**:\n",
    "\n",
    "$$\n",
    "P(A) = P(A_1) + P(A_2) = \\frac{1}{2} + \\frac{1}{4} = \\frac{3}{4}\n",
    "$$\n",
    "\n",
    "\n",
    "#### **Conditional Probability of selecting the second black ball**\n",
    "\n",
    "Given that the first selected ball is black, for the second ball to also be black, **Bag 1** must have been chosen, since only **Bag 1** contains two black balls.\n",
    "\n",
    "Thus, the probability of both events **A** and **B** (selecting the first black ball and then the second black ball) is:\n",
    "\n",
    "$$\n",
    "P(AB) = P(\\text{Choosing Bag 1}) = \\frac{1}{2}\n",
    "$$\n",
    "\n",
    "\n",
    "#### **Applying Conditional Probability**\n",
    "\n",
    "Now, we can calculate the probability that the second ball is black, given that the first ball was black, using conditional probability:\n",
    "\n",
    "$$\n",
    "P(B \\mid A) = \\frac{P(AB)}{P(A)} = \\frac{\\frac{1}{2}}{\\frac{3}{4}} = \\frac{2}{3}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "*Therefore, the correct answer is* [d] $\\frac{2}{3}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.\n",
    "\n",
    "**Explaination**\n",
    "- The probability of drawing a red marble is \\( \\mu = 0.55 \\), so the probability of drawing a **green marble** (non-red) is \\( 1 - \\mu = 0.45 \\).\n",
    "- Since each draw is independent, the probability of drawing **10 green marbles** (i.e., no red marbles) is:\n",
    "  \n",
    "  $$\n",
    "  P(\\text{No red marbles}) = (0.45)^{10} \\approx 3.405 \\times 10^{-4}\n",
    "  $$\n",
    "\n",
    "Thus, the probability that **none of the 10 marbles** drawn are red is $3.405 \\times 10^{-4}$\n",
    "\n",
    "*Therefore, the correct answer is* [b] $3.405 \\times 10^{-4}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.\n",
    " \n",
    "The probability of drawing no red marbles in a sample of 10 (i.e., all 10 marbles are green) is P($\\nu$ = 0)\n",
    "  $$\n",
    "  P(\\nu = 0) = (0.45)^{10} \\approx 3.405 \\times 10^{-4}\n",
    "  $$\n",
    "\n",
    "- **Probability that none of the 1,000 samples have ( $\\nu$ = 0 )**:\n",
    "  Since the samples are independent, the probability that none of the 1,000 samples have ( $\\nu$ = 0 ) is:\n",
    "\n",
    "  $$\n",
    "  P(\\text{No } \\nu = 0 \\text{ in all samples}) = (1 - 3.405 \\times 10^{-4})^{1000} \\approx 0.7113688022\n",
    "  $$\n",
    "\n",
    "- **Probability that at least one sample has ( $\\nu$ = 0 )**:\n",
    "  \n",
    "  $$\n",
    "  P(\\text{At least one } \\nu = 0) = 1 - P(\\text{No } \\nu = 0 \\text{ in all samples}) = 1 - 0.7113688022 \\approx 0.2886311978\n",
    "  $$\n",
    "\n",
    "Thus, the closest answer is **[c] 0.289**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score of each hypothesis is:\n",
      "{'a': 12, 'b': 12, 'c': 12, 'd': 12}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "points = [[1, 0, 1], [1, 1, 0], [1, 1, 1]]\n",
    "\n",
    "target_functions = [\n",
    "    # \n",
    "    # Target functions have the form [label1, label2, label 2] with\n",
    "    #     \"label1\" is the label of [1, 0, 1]\n",
    "    #     \"label2\" is the label of [1, 1, 0]\n",
    "    #     \"label3\" is the label of [1, 1, 1]\n",
    "    # \n",
    "    [0, 0, 0],\n",
    "    [0, 0, 1],\n",
    "    [0, 1, 0],\n",
    "    [0, 1, 1],\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 1],\n",
    "    [1, 1, 0],\n",
    "    [1, 1, 1]\n",
    "]\n",
    "\n",
    "hypotheses = {\n",
    "    'a': [1, 1, 1], # [a] Return 1 for all 3 points \n",
    "    'b': [0, 0, 0], # [b] Return 0 for all 3 points\n",
    "    'c': [0, 0, 1], # [c] if the number of 1s in x is odd, g returns 1; if it is even, g returns 0 (because [1, 0, 1] and [1, 1, 0] have two 1's, their label are 0; and [1, 1, 1] has three 1's, so its label is 1)\n",
    "    'd': [1, 1, 0]  # [d]if the number of 1s is odd, it returns 0, otherwise returns 1\n",
    "}\n",
    "\n",
    "answer = {}\n",
    "\n",
    "for hypo in hypotheses.items():\n",
    "\n",
    "    score3, score2, score1, score0 = 0, 0, 0, 0\n",
    "\n",
    "    for target in target_functions:\n",
    "\n",
    "        match_count = np.sum(np.array(hypo[1]) == np.array(target))\n",
    "\n",
    "        if match_count == 3:\n",
    "            score3 += 1\n",
    "        elif match_count == 2:\n",
    "            score2 += 1\n",
    "        elif match_count == 1:\n",
    "            score1 += 1\n",
    "        else:\n",
    "            score0 += 1\n",
    "    answer[f'{hypo[0]}'] = score3 * 3 + score2 * 2 + score1\n",
    "\n",
    "print('The score of each hypothesis is:')\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the hypotheses have equal points, **the correct answer is [e]** They are all equivalent (equal scores for g in [a] through [d])."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-10. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import các thư viện cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Định nghĩa các hàm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm phát sinh ra `target_w`, véc-tơ tham số của $f$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_target_w():\n",
    "    \"\"\"\n",
    "    Generates target_w from two random, uniformly distributed points in [-1, 1] x [-1, 1].\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    target_w : numpy array, shape (3, 1) \n",
    "        The vector of parameters of f.\n",
    "    \"\"\"\n",
    "    # Generate two points from a uniform distribution over [-1, 1]x[-1, 1]\n",
    "    p1 = np.random.uniform(-1, 1, 2)   # np.random.uniform(a, b, size) tạo ra một mảng với các giá trị ngẫu nhiên phân phối đồng đều trong khoảng từ a đến b.\n",
    "    p2 = np.random.uniform(-1, 1, 2)\n",
    "    # Compute the target W from these two points\n",
    "    target_w = np.array([p1[1] * p2[0] - p1[0] * p2[1], p2[1] - p1[1], p1[0] - p2[0]]).reshape((-1, 1))\n",
    "    \n",
    "    return target_w\n",
    "\n",
    "# print(generate_target_w())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm phát sinh ra tập dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data(N, target_w):\n",
    "    \"\"\"\n",
    "    Generates a data set by generating random inputs and then using target_w to generate the \n",
    "    corresponding outputs.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    N : int\n",
    "        The number of examples.\n",
    "    target_w : numpy array, shape (3, 1) \n",
    "        The vector of parameters of f.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X : numpy array, shape (N, 3)\n",
    "        The matrix of input vectors (each row corresponds to an input vector); the first column of \n",
    "        this matrix is all ones.\n",
    "    Y : numpy array, shape (N, 1)\n",
    "        The vector of outputs.        \n",
    "    \"\"\"\n",
    "    X = np.random.uniform(-1, 1, (N, 2))\n",
    "    X = np.hstack((np.ones((N, 1)), X)) # Add 'ones' column\n",
    "    Y = np.sign(np.dot(X, target_w))\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "# print(generate_data(10, generate_target_w()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm chạy PLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_PLA(X, Y):\n",
    "    \"\"\"\n",
    "    Runs PLA.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array, shape (N, 3)\n",
    "        The matrix of input vectors (each row corresponds to an input vector); the first column of \n",
    "        this matrix is all ones.\n",
    "    Y : numpy array, shape (N, 1)\n",
    "        The vector of outputs.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    w : numpy array, shape (3, 1) \n",
    "        The vector of parameters of g.\n",
    "    num_iterations : int\n",
    "        The number of iterations PLA takes to converge.\n",
    "    \"\"\"\n",
    "    # Initialize weights to zeros; w is a column vector of shape (3, 1)\n",
    "    w = np.zeros((X.shape[1], 1))\n",
    "    \n",
    "    # Initialize iteration counter to track the number of iterations\n",
    "    iteration = 0\n",
    "    \n",
    "    # N is the number of samples, d is the number of features (including bias term)\n",
    "    N = X.shape[0]\n",
    "    d = X.shape[1]\n",
    "\n",
    "    # Loop until all samples are correctly classified (no errors)\n",
    "    while True:\n",
    "        errors = 0  # Count the number of errors in this iteration\n",
    "        \n",
    "        # Shuffle the dataset for random sampling (helps to avoid cycles in learning)\n",
    "        mix_id = np.random.permutation(N)\n",
    "        \n",
    "        # Go through each training sample\n",
    "        for i in range(N):\n",
    "            # Get the i-th input vector x and corresponding label y\n",
    "            x, y = X[mix_id[i]].reshape(d, 1), Y[mix_id[i]]\n",
    "            \n",
    "            # If the current prediction (sign of w.T * x) doesn't match the true label, update weights\n",
    "            if np.sign(np.dot(w.T, x)) != y:\n",
    "                w += y * x  # Update weights: w = w + y * x\n",
    "                errors += 1  # Increment error counter\n",
    "                iteration += 1  # Increment iteration counter\n",
    "        \n",
    "        # If no errors were made, stop the algorithm (convergence reached)\n",
    "        if errors == 0:\n",
    "            break\n",
    "    \n",
    "    # Return the final weights and the number of iterations taken\n",
    "    return w, iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(N):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    N : int\n",
    "        The number of training examples.\n",
    "    \"\"\"\n",
    "    num_runs = 1000\n",
    "    avg_num_iterations = 0.0 # The average number of iterations PLA takes to converge\n",
    "    avg_test_err = 0.0 # The average test error of g - the final hypothesis picked by PLA\n",
    "    \n",
    "    for r in range(num_runs):\n",
    "        # Generate target_w\n",
    "        target_w = generate_target_w()\n",
    "        \n",
    "        # Generate training set\n",
    "        X, Y = generate_data(N, target_w)\n",
    "        \n",
    "        # Run PLA to pick g\n",
    "        w, num_iterations = run_PLA(X, Y)\n",
    "        \n",
    "        # Generate test set\n",
    "        X_test, Y_test = generate_data(10000, target_w)\n",
    "        \n",
    "        # Test g\n",
    "        test_err = np.mean(np.sign(np.dot(X_test, w)) != Y_test)\n",
    "        \n",
    "        # Update average values\n",
    "        avg_num_iterations += (num_iterations * 1.0 / num_runs)\n",
    "        avg_test_err += (test_err * 1.0 / num_runs)\n",
    "    \n",
    "    # Print results\n",
    "    print('avg_num_iterations = %f' % (avg_num_iterations))\n",
    "    print('avg_test_err = %f' % (avg_test_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chạy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_num_iterations = 10.356000\n",
      "avg_test_err = 0.108782\n"
     ]
    }
   ],
   "source": [
    "main(N=10) # We can use `main(10)`, but `main(N=10)` is clearer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Câu 7: ta thấy kết quả gần nhất với đáp án [b] 15.\n",
    "\n",
    "Câu 8: ta thấy kết quả gần nhất với đáp án [c] 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_num_iterations = 111.017000\n",
      "avg_test_err = 0.012998\n"
     ]
    }
   ],
   "source": [
    "main(N=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Câu 9: ta thấy kết quả gần nhất với đáp án [b] 100\n",
    "\n",
    "Câu 10: ta thấy kết quả gần nhất với đáp án [b] 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "[1] Bài 9: Perceptron Learning Algorithm. Truy cập [Machine Learning cơ bản](https://machinelearningcoban.com/2017/01/21/perceptron/)\n",
    "\n",
    "[2] Caltech (n.d.). *Homework 1 Solution*. California Institute of Technology. Truy cập: [Solution](https://work.caltech.edu/homework/hw1_sol.pdf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
